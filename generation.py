# import pickle
#
# import torch
# import numpy as np
# from music21 import instrument, note, chord, stream
#
# from main import prepare_sequences
# from networks import SentimentNet
#
#
# def generate():
#     """ Generate a piano midi file """
#     # load the notes used to train the model
#     with open('all_notes', 'rb') as filepath:
#         notes = pickle.load(filepath)
#
#     # get amount of pitch names
#     n_vocab = len(set(notes))
#     # get all pitch names
#     pitchnames = sorted(set(item for item in notes))
#     network_input, _ = prepare_sequences(notes)
#
#     net = SentimentNet(n_vocab)
#     net.load_state_dict(torch.load("piano_lstm_chopin.pth"))
#
#     num_files = 10
#     for i in range(num_files):
#         prediction_output = generate_notes(net, network_input, pitchnames)
#         create_midi(prediction_output, file_suffix=i)
#
#
# def generate_notes(model, network_input, pitchnames):
#     """ Generate notes from the neural network based on a sequence of notes """
#     # pick a random sequence from the input as a starting point for the prediction
#     start = np.random.randint(0, len(network_input) - 1)
#
#     int_to_note = dict((number, note) for number, note in enumerate(pitchnames))
#
#     pattern = list(network_input[start])
#     prediction_output = []
#
#     # copy model to computation device and set to evaluation mode
#     device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#     model = model.to(device)
#     model.eval()
#
#     # generate notes
#     notes_to_generate = 300
#     for i in range(notes_to_generate):
#         input_tensor = torch.tensor(pattern).view(1, -1).to(device)
#         next_note_distribution = model(input_tensor)
#         _, next_note_index = torch.max(next_note_distribution, dim=1)
#
#         next_note_index = next_note_index.cpu().item()
#         next_note = int_to_note[next_note_index]
#
#         prediction_output.append(next_note)
#         pattern.append(next_note_index)
#         pattern = pattern[1:]  # advance to next prediction using generated node
#
#     return prediction_output
#
#
# def create_midi(prediction_output, file_suffix):
#     """ convert the output from the prediction to notes and create a midi file
#         from the notes """
#     offset = 0
#     output_notes = []
#
#     # create note and chord objects based on the values generated by the model
#     for pattern in prediction_output:
#         # pattern is a chord
#         if ('.' in pattern) or pattern.isdigit():
#             notes_in_chord = pattern.split('.')
#             notes = []
#             for current_note in notes_in_chord:
#                 new_note = note.Note(int(current_note))
#                 new_note.storedInstrument = instrument.Piano()
#                 notes.append(new_note)
#             new_chord = chord.Chord(notes)
#             new_chord.offset = offset
#             output_notes.append(new_chord)
#         # pattern is a note
#         else:
#             new_note = note.Note(pattern)
#             new_note.offset = offset
#             new_note.storedInstrument = instrument.Piano()
#             output_notes.append(new_note)
#
#         # increase offset each iteration so that notes do not stack
#         offset += 0.5
#
#     midi_stream = stream.Stream(output_notes)
#
#     midi_stream.write('midi', fp=f'test_output_{file_suffix}.mid')
