import pickle

import torch
import torch.nn.functional as F
from music21 import instrument, note, chord, stream

import data_preprocess
import networks


def generate(train_data_path, trained_model_path, num_output_files):
    """ Generate a piano midi file """
    # load the notes used to train the model

    train_data = data_preprocess.load_from_pickle(train_data_path)
    training_notes = train_data["data"]
    note_translator = train_data["note_translator"]

    net = networks.TransformerNet.load_checkpoint(trained_model_path)

    for i in range(num_output_files):
        prediction_output = generate_notes(net, training_notes, note_translator)
        create_midi(prediction_output, file_suffix=i)


def generate_notes(model, training_notes, note_translator):
    """ Generate notes from the neural network based on a sequence of notes """

    # pick a random sequence from the input as a starting point for the prediction
    sequence_length = model.sequence_length
    note_reverse_translator = {index: note for note, index in note_translator.items()}
    init_state = data_preprocess.prepare_predict_init_state(training_notes, sequence_length)

    # create lists for model samples and actual notes created
    X = [note_translator[note] for note in init_state]
    prediction_output = init_state

    # copy model to computation device and set to evaluation mode
    device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()

    # generate notes
    notes_to_generate = 2
    for i in range(notes_to_generate):
        input_tensor = torch.tensor(X).view(1, -1).to(device)
        next_note_logits = model(input_tensor)[:, :, :]  # take only next character not entire sequence

        next_notes_index = sample_prediction(next_note_logits, "")
        # _, next_note_index = torch.max(next_note_logits, dim=2)
        # next_note_index = next_note_index.cpu().item()

        next_notes = [note_reverse_translator[note] for note in next_notes_index.cpu().numpy().flatten()]
        # next_note = note_reverse_translator[next_note_index]
        prediction_output += next_notes
        X += list(next_notes_index.cpu().numpy().flatten())
        X = X[sequence_length:]  # advance to next prediction using generated note

    return prediction_output


def sample_prediction(logits: torch.tensor, method: str, k=20) -> int:

    # if method == "top_k":
    #     logits_args = torch.argsort(logits, dim=1, descending=True)
    #     logits_args = logits_args[:, k+1:]
    #     logits[:, logits_args] = float('-inf')

    # sample from distribution vector as a multinomial distribution
    logits = logits.view(logits.size()[1:])
    dist = F.softmax(logits, dim=1)
    next_notes_index = torch.multinomial(dist, 1)

    return next_notes_index


def create_midi(prediction_output, file_suffix):
    """ convert the output from the prediction to notes and create a midi file
        from the notes """
    offset = 0
    output_notes = []

    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 0.5

    midi_stream = stream.Stream(output_notes)

    midi_stream.write('midi', fp=f'test_output_{file_suffix}.mid')
